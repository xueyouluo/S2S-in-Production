# S2S-in-Production

## 目录

<!-- TOC -->

- [S2S-in-Production](#s2s-in-production)
  - [目录](#目录)
  - [引言](#引言)
  - [OOV/UNK问题](#oovunk问题)
    - [词表改进](#词表改进)
      - [基于字的词表](#基于字的词表)
      - [基于词+字的词表](#基于词字的词表)
      - [基于sub-word-units](#基于sub-word-units)
    - [模型改进](#模型改进)
      - [强制不生成UNK](#强制不生成unk)
      - [Copy机制](#copy机制)
  - [重复问题【占坑】](#重复问题占坑)
  - [多样性问题【占坑】](#多样性问题占坑)

<!-- /TOC -->

## 引言

分享一些S2S在实际生成的应用中遇到的问题和解决方法。**欢迎大家发PR补充自己的意见。**

> - 这里主要是关于生成相关的任务，所以其他的任务可能不适用。
>
> - 文档会不断完善，但仍然会有很多分析和数据可能会缺失【各种坑】（懒-_-），大家就取其精华去其糟粕吧。

默认大家已经比较熟悉S2S架构，并且跑过一些实验了，所以不做过多的介绍，直接进入正题吧。

## OOV/UNK问题

NLP中的神经网络模型都是固定词表大小的，遇到OOV的词我们一般给它们一个特殊的标识符`<unk>`，这样就导致模型在预测阶段有时候会输出`<unk>`字符，尤其是词表越小的时候情况越严重。在NLG任务中，我们一般是不愿意模型生成`<unk>`的，因此我们需要想办法避免这种情况。

我们的解决方法可以分成两类，一类是针对词表本身进行改进，另外一类针对模型进行改进（如使用copy等）。

### 词表改进

#### 基于字的词表

比较简单的想法就是不用词，而是换成字来训练模型，一般我们中文的字的数量级也就在几千左右。采用字的词表可以大大减少模型的复杂度，但是问题也是显而易见的，一个就是输入或者输出的长度会大大增加，其次使用字的话我们就丢失了词的信息，需要模型自己从数据中学到字的组合。一般在s2s中大家还是比较少用完全基于字的模型的，主要应该也是字的模型效果没有词的好。

但是有些情况下我们还是用了字的模型，比如古诗、对联生成等任务，主要是认为古文中一般一个字就蕴含了词的信息，或者本身输入输出就不长任务较简单。一些语言模型也是基于字做的。

> 当然目前bert中文的模型也就是用字的，效果也很好

#### 基于词+字的词表

比较多的论文中为了解决OOV的问题将词表变成两个，一个基于词的，一个是基于字的，然后将两者结合起来使用。

比较常见的做法是：

- 对于一个词，如果在词表中，那么可以直接拿到它词的embedding，不在的话我们可以拿unknown的embedding或者直接取0向量。
- 同时也会将这个词中所有的字去获取字的embedding，再将字的embedding通过cnn或者rnn的方法变成一个embedding，与词的embedding拼接或者相加。这种方法的好处既可以用上词的信息，而oov的词又可以利用到它里面字的信息来得到词信息的近似。

- 同样的，在生成的时候，如果生成了`<unk>`就会退化成基于字去生成词。

前面的方法理论上来说确实比较好，但是需要涉及到修改模型的输入输出结构，涉及到的改动较大。可以采取一种比较简化的版本（本人懒-_-），也即是：

- 词和字使用同一份词表，取top-K个词+top-M个字共同组成。
- 对数据处理的时候遇到OOV的词就直接把它拆成字，这样训练和测试的时候都不会遇到`<unk>`，我们期望模型能够学到字的组合信息。
- 这种方法训练出来的模型基本上没有出现`<unk>`的词。但是整体效果并没有提升。

> 这里我突然发现，我们在训练的时候应该随机把一些非OOV的词也拆成字，这样模型才能更好的学习到字的组合信息。这里留个坑，有兴趣的话可以做做实验。

#### 基于sub-word-units

这个应该是目前最常见的方案了。这个好处是把词切分成一些更细粒度的词，但是比字的粒度又更大一些，因此可以认为它是上一种方法在词和字之间的一个平衡，比词信息少但比字的信息又多。这种方法与上面类似，也是遇到OOV的词时候用子词来组成它，但是极端情况还是会出现都拆分成字的情况。

记得在NMT的一篇论文【占坑】中使用了BPE的方法，将词拆分成子词，在翻译任务中对bleu有提高。英语法语这些语言由于有词缀等，所以可以分成子词共享一下信息，效果会比较好。

这里sub-word-units也是有很多不同的实现方法的，推荐google的[sentencepiece](<https://github.com/google/sentencepiece>)工具，提供了多种实现，大家可以自行尝试。目前我用的基本是里面的unigram算法。

对于中文来说，有两种方式来使用sentencepiece，一种就是直接不分词将整个字符串丢进去，模型自己学组合（这里有时候会出现一些比较奇怪的词，有兴趣的同学可以看看它的算法，估计也可以用来做新词发现或者分词【又留坑】）；另外一种是分词后再调用sentencepiece，大家可以比较一下二者的效果【留坑】，个人记得应该是差别不大，有时候前面一种方法效果还更好。

### 模型改进

#### 强制不生成UNK

语言的表达是多种多样的，所以我们相信即使要使用到OOV的时候也可以换个说法表达出一样的意思。那么最简单直接的方法就是在S2S做inference的时候，我们改变词表的概率分布，将UNK这个词的概率设置成0（一般通过把UNK的logits减去一个非常大的数实现），这样我们就很happy的看到没有UNK出现了。

#### Copy机制

当然前面那个方法是比较暴力的，但是个人觉得简单的才是有效的。大家研究比较多的还有就是copy机制了，思想其实很简单，对于输入中一些不在词表的词，我们是可以直接把它copy到输出中去的，比如有一些专有名词、一些数值内容，直接靠生成的话容易出现UNK或者不准确的情况。

这里列出两篇论文大家可以去参考：

1. CopyNet -[Incorporating Copying Mechanism in Sequence-to-Sequence Learning](<https://arxiv.org/pdf/1603.06393.pdf>)
2.  Point-Generator - [Get To The Point: Summarization with Pointer-Generator Networks](<https://arxiv.org/pdf/1704.04368.pdf>)

这里就不展开讲解这两篇论文了【占个坑吧】，个人觉得point-generator的思想比较简单，实现起来也更方便，而copynet就比较麻烦些，所以用PG就好了。

如果大家看过PG的源代码或者自己尝试实现过的话，可以发现其实你还要额外维护一个OOV的词表用来做copy用，这个实现起来还挺不方便的，可以采用一个简化的版本（是的，又来偷懒了-_-）。前面提到过sub-word-units，因此我们可以保证输入的词不会被转换成UNK的（当然有例外，比如训练spm的时候没见过的字），那么我们就不需要维护那个额外的OOV词表了，直接copy输入的词就好了，这样encoder这边就不需要改动了，只需要改动decoder这边的生成词表概率这部分就好了。

## 重复问题【占坑】

## 多样性问题【占坑】

