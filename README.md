# S2S-in-Production

## 目录

<!-- TOC -->

- [S2S-in-Production](#s2s-in-production)
  - [目录](#目录)
  - [引言](#引言)
  - [框架选择](#框架选择)
    - [Tensorflow流](#tensorflow流)
      - [[OpenNMT-tf](https://github.com/OpenNMT/OpenNMT-tf/)](#opennmt-tfhttpsgithubcomopennmtopennmt-tf)
      - [NMT](#nmt)
      - [Tensor2Tensor](#tensor2tensor)
      - [OpenSeq2Seq](#openseq2seq)
      - [rax](#rax)
      - [Texar](#texar)
    - [PyTorch流](#pytorch流)
    - [MXNet流](#mxnet流)
    - [统计流](#统计流)
  - [OOV/UNK问题](#oovunk问题)
    - [词表改进](#词表改进)
      - [基于字的词表](#基于字的词表)
      - [基于词+字的词表](#基于词字的词表)
      - [基于sub-word-units](#基于sub-word-units)
    - [模型改进](#模型改进)
      - [强制不生成UNK](#强制不生成unk)
      - [Copy机制](#copy机制)
  - [重复问题](#重复问题)
  - [多样性问题](#多样性问题)
  - [生成质量的评估](#生成质量的评估)
  - [生成的可控性](#生成的可控性)
  - [预训练的有效性](#预训练的有效性)
  - [多样性的输入](#多样性的输入)

<!-- /TOC -->

## 引言

分享一些S2S在实际生成的应用中遇到的问题和解决方法。**欢迎大家发PR补充自己的意见。**

> - 这里主要是关于生成相关的任务，所以其他的任务可能不适用。
>- 文档会不断完善，但仍然会有很多分析和数据可能会缺失【各种坑】（懒-_-），大家就取其精华去其糟粕吧。
> - 这些是我们团队在使用S2S时的一些经验和总结，总会存在一些错误或者纰漏，所以如果有更好的经验欢迎分享。

默认大家已经比较熟悉S2S架构，并且跑过一些实验了，所以不做过多的介绍，直接进入正题吧。当然，如果本文受欢迎的话还是可以补充各种基础知识的😆

## 框架选择

对于刚开始学习S2S的同学，我是建议自己去实现一下基本的S2S的模型的，gay佬交友网站上已经有一堆人用pytorch和tensorflow实现了各种S2S的模型，有兴趣的可以去看看。当然也可以看我两三年前参考各种别人实现写的[seq2seq](https://github.com/xueyouluo/my_seq2seq)（不过还是上古时代的LSTM的那套😂）。

对于专业人士来说，手撸一套s2s框架肯定毫无压力，但是对于我这种比较懒的人来说，直接用别人写的改改还是香😁。所以这里介绍几个S2S框架（当然，很多我也只是知道个名字😂）推荐给大家，大家各取所需。欢迎━(*｀∀´*)ノ亻!补充。

当然这样再稍微探讨一下框架的优劣，我司有同学喜欢自己维护一套代码（程序员都喜欢从头来一遍），这样的好处是对这套代码非常熟悉，用起来改起来都非常方便。但是坏处就是其他同学想用你的代码就有学习成本，而且你加的新feature也没法跟别的框架的feature合起来用。框架的好处是大家用的多了就比较熟悉了，都可以在上面加新feature，方便共享成果，但坏处就是框架一般就不太灵活，想加新功能可能得改到好几处地方（当然，牛逼的框架做的好还是只要改一点点地方）。我是建议一个团队或者一个组的同学在做S2S任务的时候都是共同维护一套框架的，而且每位同学都要非常熟悉源代码，这样工作效率会更高。

### Tensorflow流

#### [OpenNMT-tf](https://github.com/OpenNMT/OpenNMT-tf/)

#### NMT

https://github.com/tensorflow/nmt

https://github.com/google/seq2seq

#### Tensor2Tensor

https://github.com/tensorflow/tensor2tensor

#### OpenSeq2Seq

https://github.com/NVIDIA/OpenSeq2Seq

#### rax

#### Texar

https://github.com/asyml/texar

### PyTorch流

https://github.com/OpenNMT/OpenNMT-py

https://github.com/pytorch/fairseq

### MXNet流

https://github.com/awslabs/sockeye

### 统计流

http://www.statmt.org/moses/


## OOV/UNK问题

NLP中的神经网络模型都是固定词表大小的，遇到OOV的词我们一般给它们一个特殊的标识符`<unk>`，这样就导致模型在预测阶段有时候会输出`<unk>`字符，尤其是词表越小的时候情况越严重。在NLG任务中，我们一般是不愿意模型生成`<unk>`的，因此我们需要想办法避免这种情况。

我们的解决方法可以分成两类，一类是针对词表本身进行改进，另外一类针对模型进行改进（如使用copy等）。

### 词表改进

#### 基于字的词表

比较简单的想法就是不用词，而是换成字来训练模型，一般我们中文的字的数量级也就在几千左右。采用字的词表可以大大减少模型的复杂度，但是问题也是显而易见的，一个就是输入或者输出的长度会大大增加，其次使用字的话我们就丢失了词的信息，需要模型自己从数据中学到字的组合。一般在s2s中大家还是比较少用完全基于字的模型的，主要应该也是字的模型效果没有词的好。

但是有些情况下我们还是用了字的模型，比如古诗、对联生成等任务，主要是认为古文中一般一个字就蕴含了词的信息，或者本身输入输出就不长任务较简单。一些语言模型也是基于字做的。

> 当然目前bert中文的模型也就是用字的，效果也很好

#### 基于词+字的词表

比较多的论文中为了解决OOV的问题将词表变成两个，一个基于词的，一个是基于字的，然后将两者结合起来使用。

比较常见的做法是：

- 对于一个词，如果在词表中，那么可以直接拿到它词的embedding，不在的话我们可以拿unknown的embedding或者直接取0向量。
- 同时也会将这个词中所有的字去获取字的embedding，再将字的embedding通过cnn或者rnn的方法变成一个embedding，与词的embedding拼接或者相加。这种方法的好处既可以用上词的信息，而oov的词又可以利用到它里面字的信息来得到词信息的近似。

- 同样的，在生成的时候，如果生成了`<unk>`就会退化成基于字去生成词。

前面的方法理论上来说确实比较好，但是需要涉及到修改模型的输入输出结构，涉及到的改动较大。可以采取一种比较简化的版本（本人懒-_-），也即是：

- 词和字使用同一份词表，取top-K个词+top-M个字共同组成。
- 对数据处理的时候遇到OOV的词就直接把它拆成字，这样训练和测试的时候都不会遇到`<unk>`，我们期望模型能够学到字的组合信息。
- 这种方法训练出来的模型基本上没有出现`<unk>`的词。但是整体效果并没有提升。

> 这里我突然发现，我们在训练的时候应该随机把一些非OOV的词也拆成字，这样模型才能更好的学习到字的组合信息。这里留个坑，有兴趣的话可以做做实验。

#### 基于sub-word-units

这个应该是目前最常见的方案了。这个好处是把词切分成一些更细粒度的词，但是比字的粒度又更大一些，因此可以认为它是上一种方法在词和字之间的一个平衡，比词信息少但比字的信息又多。这种方法与上面类似，也是遇到OOV的词时候用子词来组成它，但是极端情况还是会出现都拆分成字的情况。

记得在NMT的一篇论文【占坑】中使用了BPE的方法，将词拆分成子词，在翻译任务中对bleu有提高。英语法语这些语言由于有词缀等，所以可以分成子词共享一下信息，效果会比较好。

这里sub-word-units也是有很多不同的实现方法的，推荐google的[sentencepiece](<https://github.com/google/sentencepiece>)工具，提供了多种实现，大家可以自行尝试。目前我用的基本是里面的unigram算法。

对于中文来说，有两种方式来使用sentencepiece，一种就是直接不分词将整个字符串丢进去，模型自己学组合（这里有时候会出现一些比较奇怪的词，有兴趣的同学可以看看它的算法，估计也可以用来做新词发现或者分词【又留坑】）；另外一种是分词后再调用sentencepiece，大家可以比较一下二者的效果【留坑】，个人记得应该是差别不大，有时候前面一种方法效果还更好。

### 模型改进

#### 强制不生成UNK

语言的表达是多种多样的，所以我们相信即使要使用到OOV的时候也可以换个说法表达出一样的意思。那么最简单直接的方法就是在S2S做inference的时候，我们改变词表的概率分布，将UNK这个词的概率设置成0（一般通过把UNK的logits减去一个非常大的数实现），这样我们就很happy的看到没有UNK出现了。

#### Copy机制

当然前面那个方法是比较暴力的，但是个人觉得简单的才是有效的。大家研究比较多的还有就是copy机制了，思想其实很简单，对于输入中一些不在词表的词，我们是可以直接把它copy到输出中去的，比如有一些专有名词、一些数值内容，直接靠生成的话容易出现UNK或者不准确的情况。

这里列出两篇论文大家可以去参考：

1. CopyNet -[Incorporating Copying Mechanism in Sequence-to-Sequence Learning](<https://arxiv.org/pdf/1603.06393.pdf>)
2.  Point-Generator - [Get To The Point: Summarization with Pointer-Generator Networks](<https://arxiv.org/pdf/1704.04368.pdf>)

这里就不展开讲解这两篇论文了【占个坑吧】，个人觉得point-generator的思想比较简单，实现起来也更方便，而copynet就比较麻烦些，所以用PG就好了。

如果大家看过PG的源代码或者自己尝试实现过的话，可以发现其实你还要额外维护一个OOV的词表用来做copy用，这个实现起来还挺不方便的，可以采用一个简化的版本（是的，又来偷懒了-_-）。前面提到过sub-word-units，因此我们可以保证输入的词不会被转换成UNK的（当然有例外，比如训练spm的时候没见过的字），那么我们就不需要维护那个额外的OOV词表了，直接copy输入的词就好了，这样encoder这边就不需要改动了，只需要改动decoder这边的生成词表概率这部分就好了。

> 这里补充一点，我们组兴华同学提到用transformer实现pg的时候，copy attention部分最好使用multi-head attention的结果（你可以取其中一个头的结果就好），自己另外算一个attention的话模型比较难收敛。
>
> 再补充一点😂，如果你自己预训练了T5模型的话，其实可以不用太担心OOV的问题，因为T5模型本身就比较倾向于copy的，所以spm + T5 + 强制不生成UNK基本也就够用了。

## 重复问题

## 多样性问题

## 生成质量的评估

## 生成的可控性

## 预训练的有效性

## 多样性的输入